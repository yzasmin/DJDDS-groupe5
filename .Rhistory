leverages <- hatvalues(model_with_outlier)
# Détection des points avec un levier important
p <- length(coef(model_with_outlier)) - 1  # Nombre de variables explicatives
n <- nrow(datasetB)  # Nombre total d'observations
lever_threshold <- 2 * (p + 1) / n  # Seuil typique
influential_leverages <- which(leverages > lever_threshold)
print(influential_leverages)
# Calcul des leviers
leverages <- hatvalues(model_with_outlier)
# Détection des points avec un levier important
p <- length(coef(model_with_outlier)) - 1  # Nombre de variables explicatives
n <- nrow(datasetB)  # Nombre total d'observations
lever_threshold <- 2 * (p + 1) / n  # Seuil typique
influential_leverages <- which(leverages > lever_threshold)
print(influential_leverages)
print(2 * (p + 1) / n)
# Calcul des leviers
leverages <- hatvalues(model_with_outlier)
# Détection des points avec un levier important
p <- length(coef(model_with_outlier)) - 1  # Nombre de variables explicatives
n <- nrow(datasetB)  # Nombre total d'observations
lever_threshold <- 2 * (p + 1) / n  # Seuil typique
influential_leverages <- which(leverages > lever_threshold)
print(influential_leverages)
print(lever_threshold)
# Calcul de la distance de Cook
cook_distances <- cooks.distance(model_with_outlier)
# Détection des points influents
influential_cook <- which(cook_distances > 1)  # Seuil typique pour Cook's Distance
print(paste("La distance de cook est supérieur à 1 pour le point :",influential_cook))
# Tracé du graphe
plot(cook_distances,
type = "h",  # Histogramme vertical
main = "Distance de Cook",
ylab = "Valeur",
xlab = "Observations",
col = "blue",
lwd = 2)
# Modèle sans le point atypique
dataset_without_outlier <- datasetB[-nrow(datasetB), ]  # Suppression du dernier point
model_without_outlier <- lm(Y ~ X, data = dataset_without_outlier)
# Comparaison des coefficients
coef_with_outlier <- coef(model_with_outlier)
coef_without_outlier <- coef(model_without_outlier)
print(coef_with_outlier)
print(coef_without_outlier)
# Différence entre les deux ensembles de coefficients
diff_coefs <- coef_with_outlier - coef_without_outlier
print(diff_coefs)
# Graphique de dispersion des résidus
par(mfrow = c(1, 2))
# Résidus avec le point 100
plot(residuals(model_with_outlier), main = "Résidus avec point atypique",
xlab = "Indice", ylab = "Résidus",
pch = 16, col = "blue")
# Résidus sans le point 100
plot(residuals(model_without_outlier), main = "Résidus sans point atypique",
xlab = "Indice", ylab = "Résidus",
pch = 16, col = "red")
# Ajouter une légende pour indiquer la différence de dispersion
legend("topright", legend = c("Avec point 100", "Sans point 100"),
col = c("blue", "red"), pch = 16)
# Initialisation du générateur de nombres aléatoires pour garantir la reproductibilité
set.seed(11)
# Paramètres du modèle
n <- 99
X <- rnorm(n, mean = 50, sd = 10)
beta_0 <- 10   # Intercept
beta_1 <- 2    # Pente
epsilon <- rnorm(n, mean = 0, sd = 5) # Génération du bruit (erreur)
Y <- beta_0 + beta_1 * X + epsilon
#Création des données dataB
datasetB <- data.frame(X, Y)
model <- lm(Y ~ X, data = datasetB)
# Résidus et levier du modèle avec le point atypique
gglm(model_with_outlier)
# Tracé des données et mise en évidence du point atypique
ggplot(datasetC, aes(x = X, y = Y)) +
geom_point(color = "blue", size = 2) +  # Points principaux
annotate("point", x = X_outlier, y = Y_outlier, color = "red", size = 3) +  # Point atypique
geom_smooth(method = "lm", se = FALSE, color = "black") +  # Régression linéaire
labs(
title = "Données avec un point atypique non influent",
x = "X",
y = "Y"
) +
theme_minimal()
X_outlier <- mean(X) + 3 * sd(X)  # En-dehors de l’intervalle X
Y_outlier <- mean(Y)  # Mais proche de la moyenne de Y pour limiter son influence
datasetC <- rbind(datasetC, data.frame(X = X_outlier, Y = Y_outlier))
# Initialisation du générateur de nombres aléatoires pour garantir la reproductibilité
set.seed(11)
# Paramètres du modèle
n <- 99
X <- rnorm(n, mean = 50, sd = 10)
beta_0 <- 10   # Intercept
beta_1 <- 2    # Pente
epsilon <- rnorm(n, mean = 0, sd = 5) # Génération du bruit (erreur)
Y <- beta_0 + beta_1 * X + epsilon
#Création des données dataB
datasetB <- data.frame(X, Y)
model <- lm(Y ~ X, data = datasetB)
# Initialisation du générateur de nombres aléatoires pour garantir la reproductibilité
set.seed(11)
# Paramètres du modèle
n <- 99
X <- rnorm(n, mean = 50, sd = 10)
beta_0 <- 10   # Intercept
beta_1 <- 2    # Pente
epsilon <- rnorm(n, mean = 0, sd = 5) # Génération du bruit (erreur)
Y <- beta_0 + beta_1 * X + epsilon
#Création des données dataB
datasetC <- data.frame(X, Y)
model <- lm(Y ~ X, data = datasetB)
X_outlier <- mean(X) + 3 * sd(X)  # En-dehors de l’intervalle X
Y_outlier <- mean(Y)  # Mais proche de la moyenne de Y pour limiter son influence
datasetC <- rbind(datasetC, data.frame(X = X_outlier, Y = Y_outlier))
# Régression avec le point atypique ajouté
model_with_outlier <- lm(Y ~ X, data = datasetB)
# Résidus et levier du modèle sans le point atypique
gglm(model)
# Résidus et levier du modèle avec le point atypique
gglm(model_with_outlier)
# Tracé des données et mise en évidence du point atypique
ggplot(datasetC, aes(x = X, y = Y)) +
geom_point(color = "blue", size = 2) +  # Points principaux
annotate("point", x = X_outlier, y = Y_outlier, color = "red", size = 3) +  # Point atypique
geom_smooth(method = "lm", se = FALSE, color = "black") +  # Régression linéaire
labs(
title = "Données avec un point atypique non influent",
x = "X",
y = "Y"
) +
theme_minimal()
# Calcul des résidus studentisés
residuals_studentized <- rstudent(model_with_outlier)
# Identifier les points atypiques
atypical_points <- which(abs(residuals_studentized) > 2)  # 2 est le seuil classique
print(paste("Le point atypique est :", atypical_points))
# Calcul des leviers
leverages <- hatvalues(model_with_outlier)
# Détection des points avec un levier important
p <- length(coef(model_with_outlier)) - 1  # Nombre de variables explicatives
n <- nrow(datasetB)  # Nombre total d'observations
lever_threshold <- 2 * (p + 1) / n  # Seuil typique
influential_leverages <- which(leverages > lever_threshold)
print(paste("Point ayant un fort levier dans ces données :",influential_leverages))
# Initialisation du générateur de nombres aléatoires pour garantir la reproductibilité
set.seed(11)
# Paramètres du modèle
n <- 99
X <- rnorm(n, mean = 50, sd = 10)
beta_0 <- 10   # Intercept
beta_1 <- 2    # Pente
epsilon <- rnorm(n, mean = 0, sd = 5) # Génération du bruit (erreur)
Y <- beta_0 + beta_1 * X + epsilon
#Création des données dataB
datasetC <- data.frame(X, Y)
model <- lm(Y ~ X, data = datasetB)
X_outlier <- mean(X) + 3 * sd(X)  # En-dehors de l’intervalle X
Y_outlier <- mean(Y)  # Mais proche de la moyenne de Y pour limiter son influence
datasetC <- rbind(datasetC, data.frame(X = X_outlier, Y = Y_outlier))
# Régression avec le point atypique ajouté
model_with_outlier <- lm(Y ~ X, data = datasetB)
# Résidus et levier du modèle avec le point atypique
gglm(model_with_outlier)
# Tracé des données et mise en évidence du point atypique
ggplot(datasetC, aes(x = X, y = Y)) +
geom_point(color = "blue", size = 2) +  # Points principaux
annotate("point", x = X_outlier, y = Y_outlier, color = "red", size = 3) +  # Point atypique
geom_smooth(method = "lm", se = FALSE, color = "black") +  # Régression linéaire
labs(
title = "Données avec un point atypique non influent",
x = "X",
y = "Y"
) +
theme_minimal()
# Calcul des leviers
leverages <- hatvalues(model_with_outlier)
# Détection des points avec un levier important
p <- length(coef(model_with_outlier)) - 1  # Nombre de variables explicatives
n <- nrow(datasetB)  # Nombre total d'observations
lever_threshold <- 2 * (p + 1) / n  # Seuil typique
influential_leverages <- which(leverages > lever_threshold)
print(paste("Point ayant un fort levier dans ces données :",influential_leverages))
# Calcul des leviers
leverages <- hatvalues(model_with_outlier)
# Détection des points avec un levier important
p <- length(coef(model_with_outlier)) - 1  # Nombre de variables explicatives
n <- nrow(datasetB)  # Nombre total d'observations
lever_threshold <- 2 * (p + 1) / n  # Seuil typique
influential_leverages <- which(leverages > lever_threshold)
print(paste("Point ayant un fort levier dans ces données :",influential_leverages))
# Calcul des résidus studentisés
residuals_studentized <- rstudent(model_with_outlier)
# Identifier les points atypiques
atypical_points <- which(abs(residuals_studentized) > 2)  # 2 est le seuil classique
print(paste("Point atypique est :", atypical_points))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gglm)
library(car)
library(MASS)
n <- 100
# Définir la matrice de covariance
sigma <- matrix(c(1, 0.92, 0.9,
0.92, 1, 0.8,
0.9, 0.8, 1),
nrow = 3)
# Générer les données corrélées
set.seed(123) # Pour reproductibilité
data <- mvrnorm(n = n, mu = c(0, 0, 0), Sigma = sigma)
df <- as.data.frame(data)
names(df) <- c("X1", "X2", "X3")
# Définir les coefficients des regresseurs
beta <- c(5, 1, 6, 3)
# Ajouter une variable dépendante y avec un bruit aléatoire
set.seed(456)
noise <- rnorm(n, mean = 0, sd = 1)
cor_mat = cor(df)
cor_mat
vp=eigen(cor_mat)
vp_r = round(vp$values,2)
#cat("Valeurs propres :",vp_r, "\n")
K= round(vp$values[1]/vp$values[length(vp$values)])
cat("K =", K)
## Ajout de Y au dataset pour calculer le VIF
df$Y <- beta[1] + df$X1 * beta[2] + df$X2 * beta[3] + df$X3 * beta[4] + noise
reg<-lm(df$Y ~ df$X1 + df$X2 + df$X3)
library(car)
cat("VIF",vif(reg))
# Effectuer l'ACP sur les variables explicatives
acp <- prcomp(df[, c("X1", "X2", "X3")], scale. = TRUE)
cat("\nVariance expliquée par les composantes principales:\n")
print(summary(acp)$importance)
# Ajouter les composantes principales au dataset
df$PC1 <- acp$x[, 1]
df$PC2 <- acp$x[, 2]
df$PC3 <- acp$x[, 3]
# Nouveau modèle de régression avec les composantes principales
reg_acp <- lm(Y ~ PC1 + PC2 + PC3, data = df)
# Résumé du modèle corrigé
cat("\nRésumé du modèle avec ACP:\n")
print(summary(reg_acp))
cor_mat <- cor(df[, c("PC1", "PC2", "PC3")])
cor_mat
vp=eigen(cor_mat)
vp_r = round(vp$values,3)
cat("Valeurs propres :",vp_r, "\n")
K= round(vp$values[1]/vp$values[length(vp$values)])
cat("K =", K)
# Initialisation du générateur de nombres aléatoires pour garantir la reproductibilité
set.seed(2)
# Paramètres du modèle
n <- 99
X <- rnorm(n, mean = 50, sd = 10)
beta_0 <- 10   # Intercept
beta_1 <- 2    # Pente
epsilon <- rnorm(n, mean = 0, sd = 5) # Génération du bruit (erreur)
Y <- beta_0 + beta_1 * X + epsilon
#Création des données dataB
datasetB <- data.frame(X, Y)
model <- lm(Y ~ X, data = datasetB)
datasetB
# Création du point atypique non influent
X_outlier <- mean(X) + 1.6 * sd(X)  # En-dehors de l’intervalle X
Y_outlier <- mean(Y)  # Mais proche de la moyenne de Y pour limiter son influence
datasetB <- rbind(datasetB, data.frame(X = X_outlier, Y = Y_outlier))
# Régression avec le point atypique ajouté
model_with_outlier <- lm(Y ~ X, data = datasetB)
# Résidus et levier du modèle sans le point atypique
gglm(model)
# Résidus et levier du modèle avec le point atypique
gglm(model_with_outlier)
# Tracé des données et mise en évidence du point atypique
ggplot(datasetB, aes(x = X, y = Y)) +
geom_point(color = "blue", size = 2) +  # Points principaux
annotate("point", x = X_outlier, y = Y_outlier, color = "red", size = 3) +  # Point atypique
geom_smooth(method = "lm", se = FALSE, color = "black") +  # Régression linéaire
labs(
title = "Données avec un point atypique non influent",
x = "X",
y = "Y"
) +
theme_minimal()
# Calcul des résidus studentisés
residuals_studentized <- rstudent(model_with_outlier)
# Identifier les points atypiques
atypical_points <- which(abs(residuals_studentized) > 2)  # 2 est le seuil classique
print(paste("Point atypique est :", atypical_points))
# Calcul des leviers
leverages <- hatvalues(model_with_outlier)
# Détection des points avec un levier important
p <- length(coef(model_with_outlier)) - 1  # Nombre de variables explicatives
n <- nrow(datasetB)  # Nombre total d'observations
lever_threshold <- 2 * (p + 1) / n  # Seuil typique
influential_leverages <- which(leverages > lever_threshold)
print(influential_leverages)
print(lever_threshold)
# Calcul de la distance de Cook
cook_distances <- cooks.distance(model_with_outlier)
# Détection des points influents
influential_cook <- which(cook_distances > 1)  # Seuil typique pour Cook's Distance
print(paste("La distance de cook est supérieur à 1 pour le point :",influential_cook))
# Tracé du graphe
plot(cook_distances,
type = "h",  # Histogramme vertical
main = "Distance de Cook",
ylab = "Valeur",
xlab = "Observations",
col = "blue",
lwd = 2)
# Modèle sans le point atypique
dataset_without_outlier <- datasetB[-nrow(datasetB), ]  # Suppression du dernier point
model_without_outlier <- lm(Y ~ X, data = dataset_without_outlier)
# Comparaison des coefficients
coef_with_outlier <- coef(model_with_outlier)
coef_without_outlier <- coef(model_without_outlier)
print(coef_with_outlier)
print(coef_without_outlier)
# Différence entre les deux ensembles de coefficients
diff_coefs <- coef_with_outlier - coef_without_outlier
print(diff_coefs)
# Initialisation du générateur de nombres aléatoires pour garantir la reproductibilité
set.seed(11)
# Paramètres du modèle
n <- 99
X <- rnorm(n, mean = 50, sd = 10)
beta_0 <- 10   # Intercept
beta_1 <- 2    # Pente
epsilon <- rnorm(n, mean = 0, sd = 5) # Génération du bruit (erreur)
Y <- beta_0 + beta_1 * X + epsilon
#Création des données dataB
datasetC <- data.frame(X, Y)
model <- lm(Y ~ X, data = datasetB)
# Initialisation du générateur de nombres aléatoires pour garantir la reproductibilité
set.seed(11)
# Paramètres du modèle
n <- 99
X <- rnorm(n, mean = 50, sd = 10)
beta_0 <- 10   # Intercept
beta_1 <- 2    # Pente
epsilon <- rnorm(n, mean = 0, sd = 5) # Génération du bruit (erreur)
Y <- beta_0 + beta_1 * X + epsilon
#Création des données dataB
datasetC <- data.frame(X, Y)
model <- lm(Y ~ X, data = datasetC)
X_outlier <- mean(X) + 3 * sd(X)  # En-dehors de l’intervalle X
Y_outlier <- mean(Y)  # Mais proche de la moyenne de Y pour limiter son influence
datasetC <- rbind(datasetC, data.frame(X = X_outlier, Y = Y_outlier))
# Régression avec le point atypique ajouté
model_with_outlier <- lm(Y ~ X, data = datasetB)
# Résidus et levier du modèle sans le point atypique
gglm(model)
# Résidus et levier du modèle avec le point atypique
gglm(model_with_outlier)
# Tracé des données et mise en évidence du point atypique
ggplot(datasetC, aes(x = X, y = Y)) +
geom_point(color = "blue", size = 2) +  # Points principaux
annotate("point", x = X_outlier, y = Y_outlier, color = "red", size = 3) +  # Point atypique
geom_smooth(method = "lm", se = FALSE, color = "black") +  # Régression linéaire
labs(
title = "Données avec un point atypique non influent",
x = "X",
y = "Y"
) +
theme_minimal()
# Calcul des résidus studentisés
residuals_studentized <- rstudent(model_with_outlier)
# Identifier les points atypiques
atypical_points <- which(abs(residuals_studentized) > 2)  # 2 est le seuil classique
print(paste("Le point atypique est :", atypical_points))
X_outlier <- mean(X) + 3 * sd(X)  # En-dehors de l’intervalle X
Y_outlier <- mean(Y)  # Mais proche de la moyenne de Y pour limiter son influence
datasetC <- rbind(datasetC, data.frame(X = X_outlier, Y = Y_outlier))
# Régression avec le point atypique ajouté
model_with_outlier <- lm(Y ~ X, data = datasetC)
# Résidus et levier du modèle sans le point atypique
gglm(model)
# Initialisation du générateur de nombres aléatoires pour garantir la reproductibilité
set.seed(11)
# Paramètres du modèle
n <- 99
X <- rnorm(n, mean = 50, sd = 10)
beta_0 <- 10   # Intercept
beta_1 <- 2    # Pente
epsilon <- rnorm(n, mean = 0, sd = 5) # Génération du bruit (erreur)
Y <- beta_0 + beta_1 * X + epsilon
#Création des données dataB
datasetC <- data.frame(X, Y)
model <- lm(Y ~ X, data = datasetC)
X_outlier <- mean(X) + 3 * sd(X)  # En-dehors de l’intervalle X
Y_outlier <- mean(Y)  # Mais proche de la moyenne de Y pour limiter son influence
datasetC <- rbind(datasetC, data.frame(X = X_outlier, Y = Y_outlier))
# Régression avec le point atypique ajouté
model_with_outlier <- lm(Y ~ X, data = datasetC)
# Résidus et levier du modèle sans le point atypique
gglm(model)
# Résidus et levier du modèle avec le point atypique
gglm(model_with_outlier)
# Tracé des données et mise en évidence du point atypique
ggplot(datasetC, aes(x = X, y = Y)) +
geom_point(color = "blue", size = 2) +  # Points principaux
annotate("point", x = X_outlier, y = Y_outlier, color = "red", size = 3) +  # Point atypique
geom_smooth(method = "lm", se = FALSE, color = "black") +  # Régression linéaire
labs(
title = "Données avec un point atypique non influent",
x = "X",
y = "Y"
) +
theme_minimal()
# Calcul des résidus studentisés
residuals_studentized <- rstudent(model_with_outlier)
# Identifier les points atypiques
atypical_points <- which(abs(residuals_studentized) > 2)  # 2 est le seuil classique
print(paste("Le point atypique est :", atypical_points))
# Calcul des leviers
leverages <- hatvalues(model_with_outlier)
# Détection des points avec un levier important
p <- length(coef(model_with_outlier)) - 1  # Nombre de variables explicatives
n <- nrow(datasetB)  # Nombre total d'observations
lever_threshold <- 2 * (p + 1) / n  # Seuil typique
influential_leverages <- which(leverages > lever_threshold)
print(paste("Point ayant un fort levier dans ces données :",influential_leverages))
# Calcul de la distance de Cook
cook_distances <- cooks.distance(model_with_outlier)
# Détection des points influents
influential_cook <- which(cook_distances > 1)  # Seuil typique pour Cook's Distance
print(paste("La distance de cook est supérieur à 1 pour le point :",influential_cook))
# Tracé du graphe
plot(cook_distances,
type = "h",  # Histogramme vertical
main = "Distance de Cook",
ylab = "Valeur",
xlab = "Observations",
col = "blue",
lwd = 2)
# Modèle sans le point atypique
dataset_without_outlier <- datasetC[-nrow(datasetB), ]  # Suppression du dernier point
model_without_outlier <- lm(Y ~ X, data = dataset_without_outlier)
# Comparaison des coefficients
coef_with_outlier <- coef(model_with_outlier)
coef_without_outlier <- coef(model_without_outlier)
print(coef_with_outlier)
print(coef_without_outlier)
# Différence entre les deux ensembles de coefficients
diff_coefs <- coef_with_outlier - coef_without_outlier
print(diff_coefs)
# Modèle sans le point atypique
dataset_without_outlier <- datasetC[-which(datasetC$ID == 97), ]  # Suppression du point 97
model_without_outlier <- lm(Y ~ X, data = dataset_without_outlier)
View(datasetC)
View(datasetC)
View(datasetC)
# Modèle sans le point atypique (point 97)
dataset_without_outlier <- datasetC[datasetC$X != 97, ]  # Suppression de l'observation où X == 97
model_without_outlier <- lm(Y ~ X, data = dataset_without_outlier)
# Comparaison des coefficients
coef_with_outlier <- coef(model_with_outlier)
coef_without_outlier <- coef(model_without_outlier)
print(coef_with_outlier)
print(coef_without_outlier)
# Différence entre les deux ensembles de coefficients
diff_coefs <- coef_with_outlier - coef_without_outlier
print(diff_coefs)
dataset_without_outlier
datasetC[datasetC$X != 97, ]
# Modèle sans l'observation de la ligne 97
dataset_without_outlier <- datasetC[-97, ]  # Suppression de l'observation numéro 97
model_without_outlier <- lm(Y ~ X, data = dataset_without_outlier)
# Comparaison des coefficients
coef_with_outlier <- coef(model_with_outlier)
coef_without_outlier <- coef(model_without_outlier)
print(coef_with_outlier)
print(coef_without_outlier)
# Différence entre les deux ensembles de coefficients
diff_coefs <- coef_with_outlier - coef_without_outlier
print(diff_coefs)_with_outlier - coef_without_outlier
# Modèle sans l'observation de la ligne 97
dataset_without_outlier <- datasetC[-97, ]  # Suppression de l'observation numéro 97
model_without_outlier <- lm(Y ~ X, data = dataset_without_outlier)
# Comparaison des coefficients
coef_with_outlier <- coef(model_with_outlier)
coef_without_outlier <- coef(model_without_outlier)
print(coef_with_outlier)
print(coef_without_outlier)
# Différence entre les deux ensembles de coefficients
diff_coefs <- coef_with_outlier - coef_without_outlier
print(diff_coefs)
# Modèle sans le point atypique
dataset_without_outlier <- datasetC[-nrow(datasetB), ]  # Suppression du dernier point
model_without_outlier <- lm(Y ~ X, data = dataset_without_outlier)
# Comparaison des coefficients
coef_with_outlier <- coef(model_with_outlier)
coef_without_outlier <- coef(model_without_outlier)
print(coef_with_outlier)
print(coef_without_outlier)
# Différence entre les deux ensembles de coefficients
diff_coefs <- coef_with_outlier - coef_without_outlier
print(diff_coefs)
# Modèle sans l'observation de la ligne 97
dataset_without_outlier <- datasetC[-97, ]  # Suppression de l'observation numéro 97
model_without_outlier <- lm(Y ~ X, data = dataset_without_outlier)
# Comparaison des coefficients
coef_with_outlier <- coef(model_with_outlier)
coef_without_outlier <- coef(model_without_outlier)
print(coef_with_outlier)
print(coef_without_outlier)
# Différence entre les deux ensembles de coefficients
diff_coefs <- coef_with_outlier - coef_without_outlier
print(diff_coefs)
